---
title: 'Mastering File Analysis in Python'
description: 'Learn how to perform file analysis in Python, from hashing files to generating organized Markdown reports. Gain insights into your file collections and identify duplicates with this versatile tool.'
image: '../../public/blogs/file-analysis.jpg'
publishedAt: '2023-10-5'
updatedAt: '2023-10-5'
author: 'Randy'
isPublished: true
tags:
  - Python
  - File Analysis
  - Data Analysis
---

# Exploring Python File Analysis Script

Welcome to a beginner-friendly guide to Python file analysis! In the realm of programming, managing files and data is a fundamental task, and understanding how to analyze and manipulate files is a valuable skill for developers. In this blog post, we'll embark on a journey through a Python script designed for precisely that purpose.

> We'll dissect each line of code, explaining its role and significance, and by the end of this journey, you'll have a clear understanding of how to analyze files using Python.

Our script not only detects duplicates but also provides insights into file sizes, types, and more. Let's dive in and explore the world of file analysis with Python, one step at a time.

# Imports and Initial Functions

In this section, we will explore the initial setup of our Python script for file analysis. We'll start by understanding the purpose of the imported modules and then delve into the details of two essential functions: `hash_file` and `format_size`.

## Imports

The success of our file analysis script heavily depends on the Python modules we import at the beginning of the script. Let's take a closer look at the three modules we import:

```python
import os
import hashlib
from datetime import datetime
```

### os Module

The `os` module, short for "operating system," is a fundamental Python module for interacting with the underlying operating system. In our script, we'll use it extensively for various file and directory operations. Some of the tasks it enables include checking file existence, manipulating file paths, and directory traversal.

### hashlib Module

The `hashlib` module is another crucial module we import. It provides a collection of secure hash functions, including MD5, SHA-1, and SHA-256, among others. In our script, we'll specifically use the MD5 hashing algorithm to calculate a unique hash for each file. These hashes will help us identify duplicate files efficiently.

### datetime Module

The `datetime` module allows us to work with dates and times, making it particularly useful for handling timestamps in our script. We'll use it to format timestamps in a human-readable format, which will provide insights into when files were last accessed or modified.

> With these three modules in place, we're equipped to perform essential file analysis tasks. Next, we'll explore the `hash_file` function and the importance of file hashing for duplicate detection.

## Calculating File Hashes

### Understanding the `hash_file` Function

The `hash_file` function plays a central role in our script by calculating a unique hash for each file. This hash serves as a fingerprint that helps us identify duplicate files efficiently. Let's break down each part of this function:

```python
def hash_file(filename):
    if not os.path.exists(filename):
        return None

    hasher = hashlib.md5()
    with open(filename, 'rb') as file:
        buf = file.read()
        hasher.update(buf)
    return hasher.hexdigest()
```

Here's how this function works:

1. **Checking File Existence**: Before proceeding with hashing, we first check if the file exists. If the file doesn't exist, we return `None` since there's no data to hash.

2. **Creating an MD5 Hash Object**: We create an MD5 hash object using `hashlib.md5()`. This object will be responsible for calculating the file's hash.

3. **Reading the File Content**: We open the file in binary read mode (`'rb'`) and read its entire content into a buffer (`buf`).

4. **Updating the Hash**: We update the MD5 hash object with the contents of the file buffer. This process incorporates the file's data into the hash calculation.

5. **Returning the Hexadecimal Hash**: Finally, we return the hexadecimal representation of the calculated hash using `hasher.hexdigest()`.

> The `hash_file` function provides a critical piece of functionality for our file analysis. It ensures that we can uniquely identify files based on their content, allowing us to detect duplicates accurately. Next, we'll explore the `format_size` function and its role in formatting file sizes appropriately.

## Understanding File Size Formatting

The `format_size` function is responsible for formatting file sizes into units such as KB, MB, or GB with two decimal places. Let's understand why this function is essential and how it accomplishes this formatting:

```python
def format_size(size_mb):
    if size_mb >= 1024:
        return f"{round(size_mb / 1024, 2)} GB"
    elif size_mb < 1:
        return f"{round(size_mb * 1000, 2)} KB"
    else:
        return f"{round(size_mb, 2)} MB"
```

Here's what the `format_size` function does:

- **Clarifying the Need for Formatting**: File sizes can vary widely, making it challenging to compare them directly. This function ensures that file sizes are presented uniformly in units that are easier to understand, such as kilobytes (KB), megabytes (MB), or gigabytes (GB).

- **Conversion to KB, MB, or GB**: Depending on the size in megabytes (`size_mb`), the function determines the appropriate unit to use. If the size is greater than or equal to 1024 MB, it converts it to gigabytes. If the size is less than 1 MB, it converts it to kilobytes. Otherwise, it keeps the size in megabytes.

- **Rounding and Formatting**: The function rounds the size to two decimal places for consistency and appends the appropriate unit. For example, if the size is 1.2345 MB, it formats it as "1.23 MB."

> With the `format_size` function, we ensure that file sizes are presented in a clear and standardized manner, making it easier to interpret the results of our file analysis.

In the following sections, we will continue to explore the remaining components of our file analysis script, including the directory analysis process and the generation of a Markdown report.

## Analyzing a Directory

In this section, we'll delve into the process of analyzing a directory and its contents using our Python script. We'll explore the `analyze_directory` function, which serves as the core of our file analysis tool. This function allows us to systematically examine each file within a specified directory, extract essential information, and detect duplicates.

### analyze_directory Function

The `analyze_directory` function is pivotal to our file analysis script. It orchestrates the entire analysis process, from traversing directories to collecting data on individual files. Below, we'll break down this function into its key components and explain their roles.

```python
def analyze_directory(directory):
    total_files = 0
    file_data = {}
    duplicate_count = {}
    file_type_counts = {}
    file_type_sizes = {}
```

#### Directory Traversal

The process begins with directory traversal using the `os.walk` function. This allows us to explore the contents of the specified directory and its subdirectories. During traversal, we maintain several variables:

- `dirpath`: The current directory path being analyzed.
- `dirnames`: A list of subdirectories within the current directory.
- `filenames`: A list of filenames within the current directory.

We also implement directory exclusion logic to skip certain directories that are typically not relevant to the analysis:

```python
    for dirpath, dirnames, filenames in os.walk(directory):
        # Skip .git directory
        if '.git' in dirnames:
            dirnames.remove('.git')

        # Skip node_modules directory
        if 'node_modules' in dirnames:
            dirnames.remove('node_modules')
```

#### Analyzing Individual Files

For each file encountered during traversal, we perform a series of steps to gather and process information:

- **Displaying the File Being Analyzed**: We print the filename to the console to keep track of progress.

- **Counting the Total Files**: We increment the `total_files` count to track the number of files analyzed.

```python
            print(f"Analyzing: {fname}")
            total_files += 1
```

- **Constructing the Full Path to the File**: We create the full path to the file by joining the `dirpath` and `fname`.

- **Extracting File Extension**: We extract the file extension using `os.path.splitext(fname)[-1]`. This helps us categorize files by type.

```python
            full_path = os.path.join(dirpath, fname)
            file_extension = os.path.splitext(fname)[-1]
```

- **Calculating File Hash**: We calculate the MD5 hash of the file using the `hash_file` function. This hash serves as a unique identifier for the file.

```python
            file_hash = hash_file(full_path)
```

- **Obtaining File Size in MB**: We determine the file's size in megabytes (MB) by retrieving the file's size in bytes and converting it.

```python
            file_size_bytes = os.path.getsize(full_path)
            file_size_mb = file_size_bytes / (1024 * 1024)
```

> The `analyze_directory` function is pivotal to our file analysis script. It orchestrates the entire analysis process, from traversing directories to collecting data on individual files.

Certainly, let's proceed with the next section and provide explanations along with code blocks. We'll also add an introductory paragraph to set the context.

## Generating a Markdown Report

In this section, we'll explore how our script generates a detailed Markdown report summarizing the file analysis results. This report will provide valuable insights into the analyzed files, their types, sizes, and any duplicate occurrences.

### write_to_md Function

The `write_to_md` function plays a pivotal role in our file analysis script. Its primary purpose is to create a Markdown report that consolidates all the data collected during the analysis. Let's dive into this function and understand how it structures the report.

```python
def write_to_md(total_files, file_data, duplicate_count, file_type_counts, file_type_sizes):
    with open('file_analysis.md', 'w') as f:
        f.write("# File Analysis\n")
        f.write(f"Total files: {total_files}\n")

        for ext, count in file_type_counts.items():
            size = format_size(file_type_sizes.get(ext, 0))
            f.write(f"{ext} files: {count} (Total Size: {size})\n")
```

> The `write_to_md` function starts by creating a Markdown document with a title and provides an overview of the total number of files and the count of each file type with their total size.

### Markdown Report Content

Now, let's explore the content that the Markdown report contains. It serves as a comprehensive summary of the file analysis results. The report includes:

- **Main heading:** "File Analysis."
- **Total files analyzed:** This section displays the overall count of files analyzed.

Next, for each file type:

- **Listing file types and their counts:** It lists each file type (e.g., `.txt`, `.jpg`) along with the count of files of that type and their total size.

For each individual file, the report includes:

- **File path:** This provides the full path to the file.
- **File size:** Displays the size of the file, formatted in KB, MB, or GB.
- **Last Accessed/Modified:** Shows the timestamp when the file was last accessed or modified.
- **Duplicate Count:** Indicates the number of times the file is duplicated in the analyzed directory.

```python
        for file_hash, data in file_data.items():
            f.write(f"## File: {data['path']}\n")
            f.write(f"- Size: {data['size']}\n")
            f.write(f"- Last Accessed/Modified: {data['last_accessed']}\n")
            f.write(f"- Duplicate Count: {duplicate_count[file_hash]}\n\n")
```

> With this structured report, users can easily review and analyze the data collected from the file analysis. Stay with us as we continue to explore the final steps of our file analysis journey.

#### here is an example of what the .md file might look like:

```md
File Analysis

Total files: 1234

.txt Files

- Count: 567
- Total Size: 23.45 MB

.jpg Files

- Count: 321
- Total Size: 45.67 MB

.pdf Files

- Count: 346
- Total Size: 12.34 MB

File: /path/to/analyzed/file1.txt

- Size: 1.23 MB
- Last Accessed/Modified: 01-15-2023 08:30
- Duplicate Count: 2

File: /path/to/analyzed/file2.jpg

- Size: 2.34 MB
- Last Accessed/Modified: 01-14-2023 14:15
- Duplicate Count: 0

File: /path/to/analyzed/file3.pdf

- Size: 3.45 MB
- Last Accessed/Modified: 01-15-2023 12:45
- Duplicate Count: 1
```

## Running the Script

In this section, we'll explore how to run our file analysis script. We'll introduce the main function, which serves as the entry point for initiating the analysis. Additionally, we'll highlight the importance of specifying the directory to be analyzed.

### main Function

```python
def main():
    directory = "/Users/randyyono/desktop/Desktop - Randy’s MacBook Pro/git-personal-projects/printify-api"
    print("Starting analysis...\n")
    total_files, file_data, duplicate_count, file_type_counts, file_type_sizes = analyze_directory(
        directory)

    write_to_md(total_files, file_data, duplicate_count,
                file_type_counts, file_type_sizes)
    print("Analysis complete!")
```

> The `main` function is the heart of our script. It acts as the entry point for executing the file analysis process. Let's delve into the role it plays and how it initiates the analysis.

**User-Specified Directory:** The `main` function begins by specifying the directory that the user intends to analyze. In our example, it's a specific directory path on the system.

**Initiating the Analysis:** After specifying the directory, the `main` function proceeds to start the analysis. It prints a message indicating that the analysis has commenced.

**Running Analysis Functions:** The `main` function then calls two essential functions:

- `analyze_directory`: This function starts the process of traversing the specified directory, collecting data about files, and identifying duplicates.
- `write_to_md`: Once the analysis is complete, this function generates a detailed Markdown report summarizing the results.

**Completion Message:** Finally, the `main` function prints a message to inform the user that the analysis is complete.

By executing the `main` function, users can easily initiate the file analysis process and generate valuable reports for further examination.

## Full Script

Now, let's bring together all the components we've explored into a complete Python script. This script performs file analysis, detects duplicates, and generates a detailed Markdown report summarizing the findings.

```python
import os
import hashlib
from datetime import datetime


def hash_file(filename):
    """Returns a hash of a file to detect duplicates."""
    if not os.path.exists(filename):  # Check if the file exists
        return None

    hasher = hashlib.md5()
    with open(filename, 'rb') as file:
        buf = file.read()
        hasher.update(buf)
    return hasher.hexdigest()


def format_size(size_mb):
    """Formats the size into appropriate units (KB, MB or GB) with two decimal places."""
    if size_mb >= 1024:
        return f"{round(size_mb / 1024, 2)} GB"
    elif size_mb < 1:
        return f"{round(size_mb * 1000, 2)} KB"
    else:
        return f"{round(size_mb, 2)} MB"


def analyze_directory(directory):
    total_files = 0
    file_data = {}
    duplicate_count = {}
    file_type_counts = {}
    file_type_sizes = {}

    for dirpath, dirnames, filenames in os.walk(directory):
        if '.git' in dirnames:
            dirnames.remove('.git')

        if 'node_modules' in dirnames:
            dirnames.remove('node_modules')

        for fname in filenames:
            print(f"Analyzing: {fname}")
            total_files += 1
            full_path = os.path.join(dirpath, fname)
            file_extension = os.path.splitext(fname)[-1]

            file_hash = hash_file(full_path)
            if file_hash is None:
                print(f"Warning: Unable to access {full_path}. Skipping.")
                continue

            file_size_bytes = os.path.getsize(full_path)
            file_size_mb = file_size_bytes / (1024 * 1024)

            file_type_counts[file_extension] = file_type_counts.get(
                file_extension, 0) + 1
            file_type_sizes[file_extension] = file_type_sizes.get(
                file_extension, 0) + file_size_mb

            time_accessed = os.path.getmtime(full_path)
            formatted_time = datetime.fromtimestamp(
                time_accessed).strftime('%m-%d-%Y %H:%M')

            if file_hash in file_data:
                duplicate_count[file_hash] += 1
            else:
                duplicate_count[file_hash] = 1
                file_data[file_hash] = {
                    "path": full_path,
                    "size": format_size(file_size_mb),
                    "last_accessed": formatted_time
                }

    return total_files, file_data, duplicate_count, file_type_counts, file_type_sizes


def write_to_md(total_files, file_data, duplicate_count, file_type_counts, file_type_sizes):
    with open('file_analysis.md', 'w') as f:
        f.write("# File Analysis\n")
        f.write(f"Total files: {total_files}\n")

        for ext, count in file_type_counts.items():
            size = format_size(file_type_sizes.get(ext, 0))
            f.write(f"{ext} files: {count} (Total Size: {size})\n")

        f.write("\n")
        for file_hash, data in file_data.items():
            f.write(f"## File: {data['path']}\n")
            f.write(f"- Size: {data['size']}\n")
            f.write(f"- Last Accessed/Modified: {data['last_accessed']}\n")
            f.write(f"- Duplicate Count: {duplicate_count[file_hash]}\n\n")

    print("File analysis has been written to file_analysis.md")


def main():
    directory = "/Users/randyyono/desktop/Desktop - Randy’s MacBook Pro/git-personal-projects/printify-api"
    print("Starting analysis...\n")
    total_files, file_data, duplicate_count, file_type_counts, file_type_sizes = analyze_directory(
        directory)

    write_to_md(total_files, file_data, duplicate_count,
                file_type_counts, file_type_sizes)
    print("Analysis complete!")


if __name__ == "__main__":
    main()
```

## Script Overview

- The script starts with essential imports, including modules for file hashing, date-time formatting, and file system operations.
- It defines four key functions:

  - `hash_file`: Calculates the MD5 hash of a file to detect duplicates.
  - `format_size`: Formats file sizes into appropriate units (KB, MB, or GB) with two decimal places.
  - `analyze_directory`: Analyzes files within a specified directory, collecting data on file types, sizes, timestamps, and duplicates.
  - `write_to_md`: Generates a Markdown report summarizing the analysis results and saves it to a file.

- The `main` function serves as the entry point, allowing users to specify the directory they want to analyze. It initiates the analysis process and calls the `write_to_md` function to create the report.

By using this script, users can gain valuable insights into their file collections, identify duplicate files, and create organized reports for further analysis.

In conclusion, we've explored the complete process of file analysis in Python, from importing essential modules to running the script and generating reports. This versatile tool can be customized to suit various file analysis needs, making it a valuable asset for managing and understanding your files.

We hope this guide has been informative and helpful as you dive into the world of file analysis with Python. Happy analyzing!
